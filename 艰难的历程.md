

- Emmm折腾了三个晚上，算是把程序跑动了。。记录一下这心酸的过程...



- 5.31 ， 晚上装好了pycharm ， pytorch准备大干一场...一抬头发现两点了..果断放弃并且滚去睡觉


  - 环境： windows10 + Spyder(python3.6) + tensorflow 1.8 + keras 2.1.6 

- 6.1 ， 
  - 晚上七点半开机准备动手。先搜了一波pytorch的例程，发现比较少，但找到不少keras的，不坚定的我改用keras做。。。

  - 先学一波pandas，把数据先导进去……经过无数小问题之后终于遇到一个最大的问题。。8000张图片我的8G内存根本放不下...提示memoryError。无奈只能砍掉四分之三的数据集，加载两千张先看看效果。(可能就是因为这个才出现谜一样的acc)

    - ```python
      #导入训练数据集
      y_train = pd.read_excel("train.xlsx")
      train_image_dir = "./train/"
      #将标签单独取出，转成独热码
      breed = pd.Series(y_train['breed'])
      one_hot = pd.get_dummies(breed, sparse = True)
      one_hot_labels = np.asarray(one_hot)
      #读取训练数据集文件名（train.xlsx的第一列）
      trainFilenames=pd.read_excel("train.xlsx", usecols=0)
      #转成numpy矩阵，再转成列表，取出文件名，组合成路径。...
      trainFilenames=trainFilenames.T.as_matrix()
      trainFilenames=trainFilenames.tolist()
      trainFilenames = [y for x in trainFilenames for y in x]
      train_img_paths = [train_image_dir + s for s in trainFilenames]
      ```

  - 初始化变量然后把图片读进来

    - ```python
      x_train = []
      train_labels = []
      im_size = 256
      img_height=im_size
      img_width=im_size
      bs = 64
      i = 0 
      for f, breed in tqdm(y_train.values):
          img = load_img(train_image_dir + '{}.jpg'.format(f), target_size=(img_height, img_width))
          #把img转成向量然后才能append到x_train
          img = np.array(img, dtype="int64")
          x_train.append(img) 
          label = one_hot_labels[i]
          train_labels.append(label)
          i += 1
      #调整格式
      y_train_raw = np.array(train_labels, np.uint8)
      x_train_raw = np.array(x_train, np.float32) / 255.0
      # 分割训练和验证数据集
      X_train, X_val, Y_train, Y_val = train_test_split(x_train_raw, y_train_raw, test_size=0.1, random_state=0)
      ```

      ​

  - 构建模型（后来这个模型被我抛弃了...）

    - ```python
      #初始化CNN
      classifier = ResNet50(include_top=False, weights='imagenet', 
                         input_shape=(img_height,img_width, 3))
      #再加几层... 
      x = Flatten(name='flatten')(classifier.output)
      x = BatchNormalization()(x)
      x = Dropout(0.8)(x)
      x = Dense(512, activation='relu')(x)
      x = BatchNormalization()(x)
      x = Dropout(0.8)(x)
      x = Dense(120, activation='softmax')(x)
      my_model = Model(inputs=classifier.input, outputs=x)
      my_model.summary()
      #编译模型
      my_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
      my_model.fit(X_train, Y_train, epochs=1,batch_size=bs, validation_data=(X_val, Y_val), verbose=1)
      ```

      ​

  - 终于可以跑了。 惊奇的看到那个一个半小时的剩余时间，欲哭无泪。突然想到可以用显卡加速，查到要先装gpu版本的tensorflow...于是我又步入了另一个大坑。

  - 安装 tensorflow-gpu 

    - cpu gpu版本不共存问题....略
    - 折腾好久，完全卸载掉所有版本再次安装gpu版本后...import之后出现 ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'
      - 解决：
        - 1、（无效）swig是一个python到c/c++的依赖包，该错误应该是缺少依赖，[安装Visual C++ Redistributable 2015 x64](https://www.microsoft.com/en-us/download/details.aspx?id=53587)即可  
        - 2、（无效） 只能使用cudnn5或者6 。 in my case, either cudnn v5 or v6 cannot work alone. I looked into the self check script, it seems that the proper installation of both cudnn64_5.dll and cudnn64_6.dll are checked:
        - 然后在tensorflow的github中找到一句话。。令我绝望 TensorFlow 1.2 may be the last time we build with cuDNN 5.1. Starting with TensorFlow 1.3, we will try to build all our prebuilt binaries with cuDNN 6.0. While we will try to keep our source code compatible with cuDNN 5.1, it will be best effort. 果断升级tensorflow..
        - ! 辣鸡网速 剩余时间四小时？？？不装了 换回CPU...otz

  - 换回CPU版本之后又出现问题。。
    - 警告：Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
      - 解决： 在代码中加入如下代码，忽略警告： 
        `import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'` 
    - 开始fit几秒种后出现kernel is death try to restart..
      - 解决：万能的...重启

  - 然后现在终于又开始跑了。。现在是2018年6月2日00:28:25

    - 提示信息是 ：192/2034 [=>............................] - ETA: 1:32:53 - loss: 7.9594 - acc: 0.0104

    - 在复制上面这条信息的时候按下Ctrl+C。 于是屏幕上愉快地出现了一行 KeyboardInterrupt ！ 从头再来 阿门。

    - 结果记录...

      - ```python
        #模型1
        classifier = ResNet50(include_top=False, weights='imagenet',
                           input_shape=(img_height,img_width, 3))
        x = Flatten(name='flatten')(classifier.output)
        x = BatchNormalization()(x)
        x = Dropout(0.25)(x)
        x = Dense(200, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.5)(x)
        x = Dense(120, activation='softmax')(x)
        my_model = Model(inputs=classifier.input, outputs=x)
        my_model.summary()
        #结果
        Epoch 1/1

         576/2034 [=>......................] - ETA: 1:06:45 - loss: 7.0753 - acc: 0.0052e+00 128/2034 [>.............................] - ETA: 1:24:51 - loss: 7.8598 - acc: 0.0078     256/2034 [>...........................] - ETA: 1:20:23 - loss: 7.6903 - acc: 0.0039 320/2034 [===>..........................] - ETA: 1:18:19 - loss: 7.5847 - acc: 0.0031
        看到这个准确率....我心痛OTZ
        并且五十多层..跑起来太慢了 果断抛弃之。
        ```

      - ```python
        #模型2
        model = Sequential()
        model.add(Conv2D(32, kernel_size=(3, 3),
                         activation='relu',
                         input_shape=(img_height,img_width, 3)))
        model.add(Conv2D(64, (3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
        model.add(Flatten())
        model.add(Dense(16, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(120, activation='softmax'))
        #结果
        Train on 2034 samples, validate on 227 samples
        Epoch 1/1
        2034/2034 [==============================] - 586s 288ms/step - loss: 5.1992 - acc: 0.0118 - val_loss: 4.7867 - val_acc: 0.0088
         
        尝试改大epoch
        #1/5
        Epoch 1/5
        2034/2034 [==============================] - 553s 272ms/step - loss: 4.7846 - acc: 0.0098 - val_loss: 4.7858 - val_acc: 0.0088...] - ETA: 4:28 - loss: 4.7849 - acc: 0.0088
        Epoch 2/5
        2034/2034 [==============================] - 566s 278ms/step - loss: 4.7823 - acc: 0.0093 - val_loss: 4.7850 - val_acc: 0.0088...] - ETA: 7:17 - loss: 4.7835 - acc: 0.0059
        Epoch 3/5
        2034/2034 [==============================] - 525s 258ms/step - loss: 4.7801 - acc: 0.0093 - val_loss: 4.7844 - val_acc: 0.0088
        Epoch 4/5
        2034/2034 [==============================] - 510s 251ms/step - loss: 4.7781 - acc: 0.0103 - val_loss: 4.7838 - val_acc: 0.0044
        Epoch 5/5
        2034/2034 [==============================] - 514s 252ms/step - loss: 4.7763 - acc: 0.0133 - val_loss: 4.7833 - val_acc: 0.0044..] - ETA: 7:00 - loss: 4.7712 - acc: 0.0187
        Emmm猜测如果多训练几次，数据更大点...也许acc能再高点？
        ```

